# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12MXPr-wp4nmFlDx1bT7TkPf8x7QFvNem
"""

from google.colab import files
uploaded = files.upload()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("heart.csv")

# Define feature matrix (X) and target variable (y)
x = df.drop('target', axis=1)
y = df['target']

# Split dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Train Naive Bayes model
model = GaussianNB()
model.fit(x_train, y_train)

# Predict on test data
y_pred = model.predict(x_test)
print(y_pred)
predict_score=accuracy_score(y_test,y_pred)
print(predict_score)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("heart.csv")

# Define feature matrix (X) and target variable (y)
x = df.drop('target', axis=1)
y = df['target']

# Split dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Train Decision Tree model
model = DecisionTreeClassifier(criterion='entropy', random_state=42)  # You can change 'gini' to 'entropy'
model.fit(x_train, y_train)

# Predict on test data
y_pred = model.predict(x_test)
print(y_pred)

# Calculate accuracy
predict_score = accuracy_score(y_test, y_pred)
print("Accuracy:", predict_score)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("heart.csv")

# Define feature matrix (X) and target variable (y)
x = df.drop('target', axis=1)
y = df['target']

# Split dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Train Random Forest model
model = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=42)  # You can change 'gini' to 'entropy'
model.fit(x_train, y_train)

# Predict on test data
y_pred = model.predict(x_test)
print(y_pred)

# Calculate accuracy
predict_score = accuracy_score(y_test, y_pred)
print("Accuracy:", predict_score)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("heart.csv")

# Define feature matrix (X) and target variable (y)
x = df.drop('target', axis=1)
y = df['target']

# Split dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Train SVM model
model = SVC(kernel='linear', random_state=42)  # You can change 'linear' to 'rbf' or 'poly' for different kernels
model.fit(x_train, y_train)

# Predict on test data
y_pred = model.predict(x_test)
print(y_pred)

# Calculate accuracy
predict_score = accuracy_score(y_test, y_pred)
print("SVM Accuracy:", predict_score)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("heart.csv")

# Define feature matrix (X) and target variable (y)
x = df.drop('target', axis=1)
y = df['target']

# Split dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Train KNN model
model = KNeighborsClassifier(n_neighbors=5)  # You can change n_neighbors to experiment with different values
model.fit(x_train, y_train)

# Predict on test data
y_pred = model.predict(x_test)
print(y_pred)

# Calculate accuracy
predict_score = accuracy_score(y_test, y_pred)
print("KNN Accuracy:", predict_score)

import numpy as np
import pandas as pd
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv("heart.csv")
x=df.drop('target',axis=1)
y=df['target']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.295,random_state=42)
# model=GaussianNB()
model = LogisticRegression()
model.fit(x_train,y_train)

y_pred = model.predict(x_test)
print(y_pred)
predict_score

